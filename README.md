## Задачи, выполненные в рамках курса Deep Learning от [DLS(МФТИ)](https://www.dlschool.org/)
## [English version](readme_eng.md)
### Основной фрэймворк: PyTorch
#### В каждом пункте ссылка на сам блокнот а также ссылка на блокнот в google colab.<br>Там удобно пользоваться навигацией по оглавлению блокнота. 
1. [Предсказание оттока пользователей. (Telco customer churn dataset) Классический ML](Churn_prediction.ipynb) ([Colab](https://colab.research.google.com/drive/1FH-85LxBdQdW8LxnRp32H20pWKSQoYIt?usp=sharing))
   * Препроцессинг данных, заполнение пропущенных значений
   * Разведочный анализ (статистики, матрицы корреляций, взаимная информация, графики)
   * Feature engineering
   * sklearn-пайплайны
   * Пишем кастомные трансформеры для препроцессинга в sklearn
   * Оверсемплинг (SMOTE, ADASYN, SMOTEENN, используем imbalanced learn библиотеку)
   * ImbLearn-пайплайны
   * Пишем функцию для удобного сравнения метрик на разных моделях
   * Используем GridSearch для подбора гиперпараметров
   * Градиентый бустинг (XGB, Catboost)
   * Стэкинг 7 разных моделей с логистической регрессией в качестве метаалгоритма.
   
3. [Классификация персонажей мультсериала "Симпсоны" (kaggle competition)](Classification_of_Simpsons_series_characters.ipynb)
   ([Colab](https://colab.research.google.com/drive/1NFBKi9QqfwxVN2pJE8rC4UUT_BXVXuBT?usp=sharing))
   * Transfer learning
   * Аугментации
   * Взвешенное сэмплирование
   * Ансамбль из трёх моделей
   * кастомный класс датасета с раширенными возможностями
   * обёртка для базового Subset, с возможностью применять к каждому сабсету свой стэк аугментаций.
   
3. [Сегментация медицинских изображений](Segmentation_of_dermatoscopic_images.ipynb) ([Colab](https://colab.research.google.com/drive/1NR6dmXTBELhtjWTmLMtOL9GoQFCKZ6Vc?usp=sharing))
   * Кастомные классы Dataset и Subset с дополнительным функционалом.
   * Аугментации на базе Albumentations
   * Два варианта SegNet (единым классом и модульный, где каждый блок реализован отдельным классом)
   * Два варианта U-Net (с MaxPooling и Upsampling, и на обычных и траспонированных свёртках с шагом 2)
   * Функция обучения и валидации с дополнительным функционалом (визуализация, sheduler и т.д.)
   * Пишем IoU с нуля в качестве метрики
   * Пишем простые лосс функции с нуля: BCE loss, DICE loss, Focal loss
   * Пишем кастомные сегментационные лоссы на базе статей с arxiv.org:<br> TotalVariation loss, Tversky loss, Lovasz-Hinge loss, Structural Similarity Loss (SSL)
   * Сравнение всех моделей и лоссов и общие выводы по всему эксперименту.
   
4. [Автокодировщики](Autoencoders.ipynb) 
   ([Colab](https://colab.research.google.com/drive/1A1zU22z4iNPuzuNLBgOyOXO_WwNxC6dZ?usp=sharing))
   * Функция сборки и предобработки данных. (скачивание, трансформы, сортировка, дополнительные атрибуты)
   * Реализация классического автокодировщика Vanilla Autoencoder в двух вариантах (линейные слои, свёртки)
   * Функция обучения и валидации с дополнительным функционалом (визуализация, sheduler и т.д.)
   * Генерация случайных изображений
   * Векторная арифметика в латентном пространстве. "Заставляем людей на фото улыбаться"
   * Реализация вариационного автокодировцика (Variational Autoencoder) в двух вариантах (линейные слои, свёртки)
   * Пишем композитный лосс для VAE (KL-дивергенция + log-likelihood)
   * Генерация случайных цифр (VAE обучаем на MNIST-датасете)
   * Реализация условного вариационного автокодировщика (Conditional VAE)
   * Генерация цифр с выбором класса(цифры) (CVAE обучаем на MNIST-датасете)
   * Сравнение распределений в латентном пространстве (понижаем t-SNE размерность до 2).
   * Реализуем Denoiser на базе VAE.
   * Реализуем распознавание лиц на базе VAE (ищем ближайшие в латентном пространстве используя косинусное сходство)
   
5. [Генеративные состязательные сети](GANs.ipynb) ([Colab](https://colab.research.google.com/drive/1JCd4wBrm6I2JA8SE5EB8j-_FUrKAEA9A?usp=sharing))
   * Кастомный класс Dataset с дополнительным функционалом (зашиваем аугментации в класс)
   * Враппер для обычного Dataloader с возвращением батча сразу на нужный device
   * Реализация архитектуры DCGAN
   * GAN-hacks (добавляем label smoothing и гауссов шум для улучшения стабильности обучения)
   * Функция обучения с дополнительными возможностями (scheduler-ы, коэффициент сглаживания и т.д.)
   * Генерация лиц
   * Оцениваем качество сгенерированных изображений по Leave One Out 1NN Classifier accuracy
   * Сравнение распределений фейковых и реальных изображений (размерность понижаем TSNE)
